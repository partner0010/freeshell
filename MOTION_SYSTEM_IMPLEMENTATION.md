# ì‚¬ì§„ â†’ ëª¨ì…˜ ê¸°ìˆ  ìŠ¤íƒ êµ¬í˜„ ì™„ë£Œ

## âœ… êµ¬í˜„ ì™„ë£Œ í•­ëª©

### 1. ëª¨ì…˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ âœ…
- `motion_schema.py`: ì™„ì „í•œ íƒ€ì… ì •ì˜
- `MotionData`: í†µí•© ëª¨ì…˜ ë°ì´í„° êµ¬ì¡°
- `ExpressionData`: í‘œì • ë°ì´í„°
- `EyeMotion`: ëˆˆ ëª¨ì…˜ (ê¹œë¹¡ì„, ì‹œì„ )
- `BreathMotion`: í˜¸í¡ ëª¨ì…˜
- `HeadMotion`: ê³ ê°œ ëª¨ì…˜
- `BodyMotion`: ëª¸ ì›€ì§ì„
- `LipSyncData`: ì…ìˆ  ë™ê¸°í™”

### 2. ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ âœ…
- `motion_pipeline.py`: ëª¨ì…˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- í”„ë¡¬í”„íŠ¸ ë¶„ì„
- AI/Rule ê¸°ë°˜ ëª¨ì…˜ ìƒì„±
- Fallback ì²˜ë¦¬

### 3. AI ê¸°ë°˜ vs ê·œì¹™ ê¸°ë°˜ ë¹„êµ âœ…
- `comparison.md`: ìƒì„¸ ë¹„êµ ë¬¸ì„œ
- ì¥ë‹¨ì  ë¶„ì„
- ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
- í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•

### 4. ìˆí¼ íŒŒì´í”„ë¼ì¸ ì—°ê³„ ë°©ì‹ âœ…
- `motion_to_video.py`: Scene ë³€í™˜
- `integration_guide.md`: í†µí•© ê°€ì´ë“œ
- Orchestrator í†µí•©
- ì „ì²´ ì›Œí¬í”Œë¡œìš°

## í•µì‹¬ ê¸°ëŠ¥

### ëª¨ì…˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ

```python
MotionData(
    image_path="character.jpg",
    duration=5.0,
    expressions=[
        ExpressionData(
            type=ExpressionType.HAPPY,
            intensity=0.8,
            duration=2.0,
            start_time=0.0
        )
    ],
    eye=EyeMotion(
        blink_interval=3.0,
        blink_duration=0.15
    ),
    breath=BreathMotion(
        cycle_duration=3.0,
        intensity=0.02
    ),
    head=HeadMotion(
        nod_enabled=True,
        duration=1.0
    )
)
```

### ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# 1. ëª¨ì…˜ ìƒì„±
pipeline = MotionPipeline()
motion_data = await pipeline.process(
    image_path="image.jpg",
    prompt="í–‰ë³µí•œ í‘œì •, ëˆˆ ê¹œë¹¡ì„, ê³ ê°œ ë„ë•ì„",
    duration=5.0,
    use_ai=True  # AI ìš°ì„ , ì‹¤íŒ¨ ì‹œ Rule
)

# 2. Scene ë³€í™˜
converter = MotionToVideoConverter()
scene = await converter.convert_to_scene(motion_data, voice_path="voice.mp3")

# 3. ì˜ìƒ ë Œë”ë§
renderer = RendererFactory.create_renderer("output.mp4")
video_path = await renderer.render([scene])
```

### AI vs Rule ë¹„êµ

| í•­ëª© | AI ê¸°ë°˜ | ê·œì¹™ ê¸°ë°˜ |
|------|---------|-----------|
| ì²˜ë¦¬ ì‹œê°„ | 5-30ì´ˆ | <1ì´ˆ |
| ë¹„ìš© | ë†’ìŒ | ì—†ìŒ |
| í’ˆì§ˆ | ë†’ìŒ | ì¤‘ê°„ |
| ìì—°ìŠ¤ëŸ¬ì›€ | ë†’ìŒ | ë‚®ìŒ |
| ì•ˆì •ì„± | ì¤‘ê°„ | ë†’ìŒ |

### ìˆí¼ íŒŒì´í”„ë¼ì¸ ì—°ê³„

```
Image â†’ MotionPipeline â†’ MotionData â†’ Scene â†’ FFmpegRenderer â†’ Video
```

## ì‚¬ìš© ì˜ˆì‹œ

```python
from orchestrator.motion import MotionPipeline, MotionToVideoConverter
from orchestrator.video import RendererFactory

# ëª¨ì…˜ ìƒì„±
pipeline = MotionPipeline()
pipeline.register_ai_engine(AIMotionEngine())
pipeline.register_rule_engine(RuleMotionEngine())

motion_data = await pipeline.process(
    image_path="character.jpg",
    prompt="í–‰ë³µí•œ í‘œì •, ëˆˆ ê¹œë¹¡ì„",
    duration=5.0
)

# Scene ë³€í™˜ ë° ë Œë”ë§
converter = MotionToVideoConverter()
scene = await converter.convert_to_scene(motion_data, "voice.mp3")

renderer = RendererFactory.create_renderer("output.mp4")
video_path = await renderer.render([scene])
```

## ìƒì„±ëœ íŒŒì¼

```
orchestrator/motion/
â”œâ”€â”€ motion_schema.py          # ëª¨ì…˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ
â”œâ”€â”€ motion_pipeline.py       # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ ai_motion_engine.py      # AI ê¸°ë°˜ ì—”ì§„
â”œâ”€â”€ rule_motion_engine.py    # ê·œì¹™ ê¸°ë°˜ ì—”ì§„
â”œâ”€â”€ motion_to_video.py       # Scene ë³€í™˜
â”œâ”€â”€ comparison.md            # AI vs Rule ë¹„êµ
â””â”€â”€ integration_guide.md     # í†µí•© ê°€ì´ë“œ
```

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… ëª¨ì…˜ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì™„ë£Œ
2. âœ… ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
3. âœ… AI vs Rule ë¹„êµ ì™„ë£Œ
4. âœ… ìˆí¼ íŒŒì´í”„ë¼ì¸ ì—°ê³„ ì™„ë£Œ
5. â­ï¸ ì‹¤ì œ AI ì„œë¹„ìŠ¤ í†µí•©
6. â­ï¸ FFmpeg ëª¨ì…˜ ì ìš© í•„í„° ê³ ë„í™”

---

**ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸ‰
